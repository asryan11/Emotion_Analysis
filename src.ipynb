{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install textblob\n",
    "%pip install beautifulsoup4\n",
    "%pip install wordcloud\n",
    "%pip install nltk\n",
    "%pip install toktok\n",
    "%pip install spacy\n",
    "%pip install pandas\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='Emotion', data=df)\n",
    "plt.title('Emotion Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.tagged import ToktokTokenizer\n",
    "tokenizers = ToktokTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseremovel_text(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  text = soup.get_text()\n",
    "  text = re.sub('\\[[^]]*\\]', '',text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(noiseremovel_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "  ps = nltk.porter.PorterStemmer()\n",
    "  text = '.'.join([ps.stem(word) for word in text.split()])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with a column 'review'\n",
    "\n",
    "# Download the necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text, stop_words=None, is_lower_case=False):\n",
    "    # Create a ToktokTokenizer instance\n",
    "    tokenizers = ToktokTokenizer()\n",
    "\n",
    "    # Use a regular expression to split the text into words while preserving consecutive full-stop signs\n",
    "    words = re.findall(r'\\w+|\\.\\.+', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    if stop_words is None:\n",
    "        stop_words = set()\n",
    "    else:\n",
    "        stop_words = set(stop_words)\n",
    "\n",
    "    if is_lower_case:\n",
    "        # Remove stopwords without converting tokens to lowercase\n",
    "        filtokens = [i for i in words if i not in stop_words]\n",
    "    else:\n",
    "        # Remove stopwords after converting tokens to lowercase\n",
    "        filtokens = [i for i in words if i.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered words back into a sentence\n",
    "    filtered_text = ' '.join(filtokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Now, apply the remove_stopwords function to the 'review' column of the DataFrame\n",
    "\n",
    "# Get the English stopwords\n",
    "stop_wr = set(stopwords.words('english'))\n",
    "\n",
    "# Apply the remove_stopwords function to the 'review' column of the DataFrame\n",
    "df['text'] = df['text'].apply(remove_stopwords, stop_words=stop_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text[:629665]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.text[629665:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0.0,max_df=1.0,binary=False,ngram_range=(1,3))\n",
    "cv_train = cv.fit_transform(X)\n",
    "cv_test = cv.fit_transform(y)\n",
    "print('Bow_cv_train',cv_train.shape)\n",
    "print('Bow_cv_test',cv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(min_df=0.0,max_df=1.0,binary=False,ngram_range=(1,3))\n",
    "tf_train = tf.fit_transform(X)\n",
    "tf_test = tf.fit_transform(y)\n",
    "print('Bow_tf_train',tf_train.shape)\n",
    "print('Bow_tf_test',tf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelBinarizer instance\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "# Fit and transform the 'sentiment' column\n",
    "emotion_encoded = label_binarizer.fit_transform(df['Emotion'])\n",
    "print(emotion_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert the text data to bag-of-words features\n",
    "X_bow = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logistic = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n",
    "\n",
    "# Fitting the model on the training data\n",
    "lr_bow = logistic.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lr_bow.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_binarizer.classes_,\n",
    "    yticklabels=label_binarizer.classes_\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='Emotion', order=df['Emotion'].value_counts().index)\n",
    "plt.title('Distribution of Emotion Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new text data\n",
    "new_text = [\"Im not crying, it's just been raining... on my face.\"]\n",
    "new_text_bow = vectorizer.transform(new_text)\n",
    "y_pred = lr_bow.predict(new_text_bow)\n",
    "\n",
    "print(\"Predicted Sentiment:\", y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text, vectorizer, model, label_encoder):\n",
    "    # Preprocess the text\n",
    "    processed_text = remove_stopwords(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_features = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probabilities = model.predict_proba(text_features)[0]\n",
    "    \n",
    "    # Get the most likely emotion\n",
    "    predicted_idx = np.argmax(probabilities)\n",
    "    predicted_emotion = label_encoder.classes_[predicted_idx]\n",
    "    prediction_probability = probabilities[predicted_idx]\n",
    "    \n",
    "    # Return prediction and probabilities\n",
    "    return predicted_emotion, prediction_probability, dict(zip(label_encoder.classes_, probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the model to a file  \n",
    "with open('ed.pkl', 'wb') as model_file:\n",
    "    # Save the model using pickle\n",
    "    pickle.dump(lr_bow, model_file)\n",
    "# Save the vectorizer to a file\n",
    "with open('vect.pkl', 'wb') as vectorizer_file:\n",
    "    # Save the vectorizer using pickle\n",
    "    pickle.dump(vectorizer, vectorizer_file)\n",
    "# Save the label binarizer to a file\n",
    "with open('lb.pkl', 'wb') as label_file:\n",
    "    # Save the label binarizer using pickle\n",
    "    pickle.dump(label_binarizer, label_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
